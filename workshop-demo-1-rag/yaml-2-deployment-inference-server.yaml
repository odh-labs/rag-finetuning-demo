kind: Deployment
apiVersion: apps/v1
metadata:
 name: hf-text-generation-inference-server
 namespace: llm-rag
 labels:
   app: hf-text-generation-inference-server
spec:
 replicas: 1
 selector:
   matchLabels:
     app: hf-text-generation-inference-server
 template:
   metadata:
     labels:
       app: hf-text-generation-inference-server
   spec:
     restartPolicy: Always
     schedulerName: default-scheduler
     terminationGracePeriodSeconds: 120
     containers:
       - resources:
           limits:
             cpu: '12'
             memory: 36Gi
             nvidia.com/gpu: '1'
           requests:
             cpu: '12'
             memory: 36Gi
             nvidia.com/gpu: '1'
         terminationMessagePath: /dev/termination-log
         name: server
         envFrom:
           - configMapRef:
               name: hf-text-generation-inference-server-configmap
           - secretRef:
               name: hf-text-generation-inference-server-secrets
         securityContext:
           runAsUser: 1000
           capabilities:
             drop:
               - ALL
           runAsNonRoot: true
           allowPrivilegeEscalation: false
           seccompProfile:
             type: RuntimeDefault
         ports:
           - name: http
             containerPort: 3000
             protocol: TCP
         imagePullPolicy: IfNotPresent
         volumeMounts:
           - name: models-cache
             mountPath: /models-cache
           - name: shm
             mountPath: /dev/shm
         terminationMessagePolicy: File
         image: 'ghcr.io/huggingface/text-generation-inference:1.1.0'
     volumes:
       - name: models-cache
         persistentVolumeClaim:
           claimName: models-cache
       - name: shm
         emptyDir:
           medium: Memory
           sizeLimit: 1Gi
     dnsPolicy: ClusterFirst